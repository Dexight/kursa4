from sklearn.naive_bayes import GaussianNB
from analysis import *
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def NaiveBayes(features, Class):
    print("\n" * 2)
    print("НАИВНЫЙ БАЙЕС")

    #=============================\
    #БЕЗ НАСТРОЙКИ ГИПЕРПАРАМЕТРОВ|
    #----------------------------/

    # Разделение данных на обучающий и тестовый наборы
    X_train, X_test, Y_train, Y_test = train_test_split(features, Class, test_size=0.2, random_state=42)

    # Создание и обучение модели наивного Байеса на обучающем наборе
    classifier = GaussianNB()
    classifier.fit(X_train, Y_train)

    # Предсказание классов для тестового набора
    predictions_test = classifier.predict(X_test)

    # Оценка производительности модели на тестовом наборе
    accuracy = accuracy_score(Y_test, predictions_test)
    print("Оценка на тестовом наборе:", accuracy)

NaiveBayes(standardized_features, df['Class'])

# Вывод: Наивный Байесовский метод работает с меньшей точностью, чем метод логистической регрессии,
# но результат >90% говорит о том, что этот метод также хорошо подходит для решения моей задачи классификации.
# Разность оценки между этими методами может быть связана с "гибкостью" метода лог. регр. (?см. ниже)


# Вопрос: Насколько я знаю, Наивный Байес не сильно чувствителен к выбросам, чего нельзя сказать об методе лог. регр.
# Плюс ко всему, в Байесе предполагается, что все признаки независимые друг от друга. 
# Может ли та слабая зависимость настолько повлиять на результат (целых 5%)?

# И если нет, то весь ответ кроется только лишь в реализации методов?
# (Например в гипперпараметре, который по-умолчанию для метода лог.регр. мне идеально подходил.)

# Просто насколько я понял, метод Наивного Байеса должен тут работать если не лучше, то на уровне метода лог.регр.

# TODO Обработать выбросы и повторить обучение модели. 